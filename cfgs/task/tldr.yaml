# @package _global_

task:
  id: tldr
  args:
    tokenizer_id: gpt2
    max_prompt_length: 500

sampling:
  batch_size_per_process: 32
  max_prompt_len: 500
  max_gen_len: 50
  prompt_padding_side: left
  prompt_truncation_side: left
  context_padding_side: right
  context_truncation_side: right
  train_generation_kwargs:
    do_sample: True
    max_new_tokens: ${sampling.max_gen_len}
  eval_generation_kwargs:
    do_sample: False
    max_new_tokens: ${sampling.max_gen_len}

reward_fn:
  id: adapter_reward
  args:
    #adapter_id: rm_adapter_checkpoint_0.7
    adapter_id: jdchang/tldr_rm_adapter
    reward_tokenizer_id: EleutherAI/gpt-j-6B

eval_metrics:
  - id: rm_model
    args: 
      tokenizer_id: EleutherAI/gpt-j-6B
      batch_size: 5
  - id: rouge
    args:
      use_single_ref: True # note there is only 1 ref
