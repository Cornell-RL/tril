# @package _global_

task:
  id: tldr
  #id: tldr_preference_rl
  args:
    #tokenizer_id: gpt2
    tokenizer_id: EleutherAI/gpt-j-6B
    #tokenizer_id: mistralai/Mistral-7B-v0.1
    #tokenizer_id: gpt2-xl
    #max_prompt_length: 500
    max_prompt_length: 512
    #reference_type: chosen

sampling:
  batch_size_per_process: 32
  #batch_size_per_process: 64
  #max_prompt_len: 500
  max_prompt_len: 512
  #max_gen_len: 50
  max_gen_len: 53
  prompt_padding_side: left
  prompt_truncation_side: left
  context_padding_side: right
  context_truncation_side: right
  train_generation_kwargs:
    do_sample: True
    max_new_tokens: ${sampling.max_gen_len}
    temperature: 0.7
  eval_generation_kwargs:
    #do_sample: False
    do_sample: True
    #top_p: 0.92
    temperature: 0.01
    max_new_tokens: ${sampling.max_gen_len}

reward_fn:
  id: adapter_reward
  args:
    #adapter_id: jdchang/tldr_rm_adapter
    adapter_id: 16_gptj_process
    #adapter_id: 16_rm_adapter_checkpoint
    reward_tokenizer_id: EleutherAI/gpt-j-6B
    #reward_tokenizer_id: gpt2-xl

eval_metrics:
  - id: rm_model
    args: 
      tokenizer_id: EleutherAI/gpt-j-6B
      #tokenizer_id: gpt2-xl
      batch_size: 5
      #batch_size: 32
  - id: rouge
    args:
      use_single_ref: True # note there is only 1 ref
  - id: causal_perplexity
    args:
      tokenizer_id: EleutherAI/gpt-j-6B
      stride: 512
      model_type: causal
