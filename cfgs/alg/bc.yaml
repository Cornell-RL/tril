alg_id: bc

imdb:
  id: bc
  build_reward: False

  args:
    seed: 0
    grad_accumulation: 1
    batch_size_per_process: 15
    n_epochs: 10
    eval_batch_size: 100 
    eval_every: 500
    save_every: 500 #TODO
    eval_zero_shot: false
    save_checkpoints: false #TODO
    eval_splits: ['val']

  optimizer:
    id: adamw
    args:
      lr: 1e-5
      weight_decay: 1e-6
      eps: 1e-5

  tokenizer:
    model_name: lvwerra/gpt2-imdb
    padding_side: left 
    truncation_side: left 
    pad_token_as_eos_token: True 

  policy:
    id: actor
    args:
      model_type: causal
      model_name: lvwerra/gpt2-imdb
      max_prompt_len: ${sampling.max_prompt_len}
      max_gen_len: ${sampling.max_gen_len}
      create_reference: False
      gen_kwargs: ${sampling.train_generation_kwargs}
      prompt_truncation_side: ${sampling.prompt_truncation_side}

commongen:
  id: bc
  build_reward: False

  args:
    seed: 0
    grad_accumulation: 1
    batch_size_per_process: 30
    n_epochs: 10
    eval_batch_size: 100 
    eval_every: 500
    save_every: 500 #TODO
    eval_zero_shot: false
    save_checkpoints: false #TODO
    eval_splits: ['val']

  optimizer:
    id: adamw
    args:
      lr: 1e-5
      weight_decay: 1e-6
      eps: 1e-5

  tokenizer:
    model_name: t5-base
    padding_side: left 
    truncation_side: left 
    pad_token_as_eos_token: False 

  policy:
    id: actor
    args:
      model_type: seq2seq
      model_name: t5-base
      max_prompt_len: ${sampling.max_prompt_len}
      max_gen_len: ${sampling.max_gen_len}
      create_reference: False
      gen_kwargs: ${sampling.train_generation_kwargs}
      prompt_truncation_side: ${sampling.prompt_truncation_side}

tldr:
  id: bc
  build_reward: False

  args:
    seed: 0
    grad_accumulation: 16
    batch_size_per_process: 2
    n_epochs: 1
    eval_batch_size: 32
    eval_every: 500
    save_every: 500 #TODO
    eval_zero_shot: false
    save_checkpoints: true #TODO
    eval_splits: ['val']
    #eval_splits: ['test']

  optimizer:
    #id: adamw
    id: adam
    args:
      lr: 1e-5
      weight_decay: 0.0
      #weight_decay: 1e-6
      eps: 1e-8
      #eps: 1e-5

  tokenizer:
    #model_name: gpt2
    #model_name: gpt2-xl
    #model_name: EleutherAI/pythia-1.4b
    #model_name: EleutherAI/gpt-neo-1.3B
    model_name: EleutherAI/gpt-j-6b
    #model_name: mistralai/Mistral-7B-v0.1
    padding_side: left
    truncation_side: right 
    pad_token_as_eos_token: True 

  policy:
    id: actor
    args:
      model_type: causal
      #model_name: CarperAI/openai_summarize_tldr_sft
      #model_name: mistralai/Mistral-7B-v0.1
      #model_name: /home/jdc396/tril/outputs/gptj-lora/2023-12-02_10-36-05/model_11500
      #model_name: /home/jdc396/tril/outputs/gptj-lora/chkpt_noquant_nolora/model_6000
      #model_name: gpt2-xl
      #model_name: EleutherAI/pythia-1.4b
      #model_name: EleutherAI/gpt-neo-1.3B
      model_name: EleutherAI/gpt-j-6b
      max_prompt_len: ${sampling.max_prompt_len}
      max_gen_len: ${sampling.max_gen_len}
      create_reference: False
      mlp_head: False
      quantize_model: False
      #quantize_model: True
      gen_kwargs: ${sampling.train_generation_kwargs}
      prompt_truncation_side: ${sampling.prompt_truncation_side}
      
      #lora:
      #  peft_config:
      #    r: 16
      #    lora_alpha: 32
      #    lora_dropout: 0.05
      #    task_type: CAUSAL_LM
